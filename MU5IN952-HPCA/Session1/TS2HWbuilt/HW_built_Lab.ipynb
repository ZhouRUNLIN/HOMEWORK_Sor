{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a4b902",
   "metadata": {},
   "source": [
    "Massive parallel programming on GPUs and applications, by Lokman ABBAS TURKI  \n",
    "\n",
    "# 2. Hello World! and built-in variables\n",
    "\n",
    "## 2.1 Objective\n",
    "\n",
    "The main purpose of this lab is to show the multi-granularity parallelization in CUDA involving blocks, threads, and warps. We show this multi-granularity using an easy example of printing built-in variables. The \"printf\" function on the device is also studied as well as other GPU specifications that can be obtained with fast \"cudaDeviceGetAttribute\". When printing built-in variables, feel free to test your own if statement.\n",
    "\n",
    "As usual, I urge students to open CUDA documentation, especially:\n",
    "\n",
    "1) the specifications of CUDA API functions within the [CUDA_Runtime_API](https://docs.nvidia.com/cuda/cuda-runtime-api/index.html).\n",
    "2) the examples of how to use the CUDA API functions in [CUDA_C_Programming_Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0394c-197a-43bf-ba2f-9cb66620604b",
   "metadata": {},
   "source": [
    "## 2.2 Content\n",
    "\n",
    "Compile HWbuilt.cu using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a6982-9107-4ae4-9456-40aebb2671a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc HWbuilt.cu -o HW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273966c6",
   "metadata": {},
   "source": [
    "\n",
    "Execute HW using (on Windows machine ./ is not needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc6c946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./HW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee0019-6b39-45c2-96ba-3fe78446cf7b",
   "metadata": {},
   "source": [
    "As long as you did not include any additional instruction in the file HWbuilt.cu, the execution above is not supposed to return any value. At least, no compilation error is detected by the compiler!\n",
    "\n",
    "In the following questions, you will need to include your own code in the file HWbuilt.cu, then compile it and execute it before answering.\n",
    "\n",
    "\n",
    "### 2.2.1 In the main function, use cudaDeviceGetAttribute and then cudaDeviceGetLimit to display \n",
    "\n",
    "a) The major Y and the minor y of the device compute capability. Explain why this is faster than using cudaGetDeviceProperties.\n",
    "\n",
    "b) The size of the FIFO buffer of the data transferred to the host when executing printf() on the device. Explain then why should we use cudaDeviceSynchronize on the host when calling printf() on the device.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f786236-8298-4e52-947d-1176006b8204",
   "metadata": {},
   "source": [
    "<b><font color='red'>In the following questions, all compilations will be performed using the compilation options\n",
    "\n",
    "-arch=compute_60 -code=sm_Yy\n",
    "\n",
    ", with Y and y from question 1.2.1.a. For example</font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae82b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc HWbuilt.cu -arch=compute_60 -code=sm_75 -o HW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b21cc-c613-49bd-8b76-e40693e27fd3",
   "metadata": {},
   "source": [
    "### 2.2.2 Printing Hello World! and built-in variables\n",
    "\n",
    "a) Fill the kernel print_k with the appropriate code to print \"Hello World!\" and then call it within the main function.\n",
    "\n",
    "b) What happens when you call print_k with two blocks and four threads per block?\n",
    "\n",
    "c) Instead of printing \"Hello World!\", print the values of threadIdx.x, blockDim.x, blockIdx.x, and gridDim.x.\n",
    "\n",
    "d) With two blocks and four threads per block, execute the code of question c) multiple times. What do you remark?\n",
    "\n",
    "\n",
    "### 2.2.3 More built-in variables printing with 64 blocks and 64 threads per block\n",
    "\n",
    "a) In print_k, write an if sentence that excludes all threads from printing except threads 0.\n",
    "\n",
    "b) Execute the code of the question a) multiple times. What do you remark?\n",
    "\n",
    "c) In print_k, write an if sentence that excludes all blocks from printing except block 0.\n",
    "\n",
    "d) Execute the code of the question c) multiple times. What do you remark?\n",
    "\n",
    "e) From questions c) and d), what is the number of threads (lanes or threads of the same warp) that execute in lockstep?\n",
    "\n",
    "f) What if you answer questions c)d)e) and compile your code with -arch=compute_Yy -code=sm_Yy, with Yy>60? What happens?\n",
    "\n",
    "\n",
    "### 2.2.4 Your index variables using built-in variables\n",
    "\n",
    "a) Defines an index variable to distinguish lanes (threads within the same warp).\n",
    "\n",
    "b) Defines an index variable to distinguish warps within one block.\n",
    "\n",
    "c) Defines an index variable to distinguish warps within the whole grid.\n",
    "\n",
    "d) In what memory space are these variables stored?\n",
    "\n",
    "e) What happens to indices in a)b)c) when we replace warps with \"n\" consecutive threads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d24601-8232-4dfa-841f-c7cdd1409167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
